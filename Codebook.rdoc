= Code Book for the wearable computing project (run_analysis.R)
This is the code book for the raw to tidy data transformation. This process is carried out by the run_analysis.R script 
that extracts raw data from different files in UCI HAR Dataset directory were the wearable computing project data is located.
In the following sections, you'll find information about de raw data such as: general description of the scenario where the data
was taken; description of the files included in the project as the description of the features or variables, data types and aknowledgements. 
It's very important to keep in mind that the information about the raw data is same as the records and files you can get from
https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip and http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones.
It was modified just for presentation affairs but the rights are described in the Licence and acknowledgement section.

For the tidy data (https://github.com/ronaldraxon/GetandCleanDat_Proj/blob/master/Codebook.rdoc#meta-data-tidy-data), you can find information about output variables and its data types. A full description of the code 
for the run_analysis.R script and what it does, will be included in the "How the run_analisys script works?" (https://github.com/ronaldraxon/GetandCleanDat_Proj/blob/master/Codebook.rdoc#how-the-run_analisys-script-works) section. 
Parameters and local variables will be included as well. 

== Meta-data (Raw data)

=== General Description (Extracted from the documentation of the data set).

The data was extracted from the url: https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
and it has multiple variable values corresponging to the "Human Activity Recognition Using Smartphones Dataset" project.
The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person 
performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone
(Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, they captured 3-axial linear acceleration
and 3-axial angular velocity at a constant rate of 50Hz. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers 
was selected for generating the training data and 30% the test data.

=== Data set files
If you look inside the UCI HAR Dataset directory, you can find the following files:

* features_info.txt 
Shows information about the variables used on the feature vector such as tBodyAcc-XYZ, tGravityAcc-XYZ, tBodyAccJerk-XYZ, among others.

* features.txt 
It contains a full list of features (561 features)

* activity_labels.txt 
It cointains a list of the activities with its corresponding codes, it allowa to link the class labels with their activity name.

* train/X_train.txt
Training set with values for the 561 features (7352 observations)

* train/y_train.txt
It contains the respecive subject code for each observation (same 7352 observations as in X_train.txt)

* test/X_test.txt 
Test set with values for the 561 features (2947 observations)

* test/y_test.txt
It contains the respecive subject code for each observation (same 2947 observations as in X_train.txt)

The following files are available for the train and test data. Their descriptions are equivalent.

* train/subject_train.txt 
Each row identifies the subject who performed the activity for each window sample. Its range is from 1 to 30. 

* train/Inertial Signals/total_acc_x_train.txt
The acceleration signal from the smartphone accelerometer X axis in standard gravity units 'g'. Every row shows a 128 element vector. The same description applies for the 'total_acc_x_train.txt' and 'total_acc_z_train.txt' files for the Y and Z axis. 

* train/Inertial Signals/body_acc_x_train.txt 
The body acceleration signal obtained by subtracting the gravity from the total acceleration. 

* train/Inertial Signals/body_gyro_x_train.txt 
The angular velocity vector measured by the gyroscope for each window sample. The units are radians/second. 

=== Features
The features selected for this database come from the accelerometer and gyroscope 3-axial raw signals tAcc-XYZ and tGyro-XYZ. These time domain signals (prefix 't' to denote time) were captured at a constant rate of 50 Hz. Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. Similarly, the acceleration signal was then separated into body and gravity acceleration signals (tBodyAcc-XYZ and tGravityAcc-XYZ) using another low pass Butterworth filter with a corner frequency of 0.3 Hz. 
Subsequently, the body linear acceleration and angular velocity were derived in time to obtain Jerk signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ). Also the magnitude of these three-dimensional signals were calculated using the Euclidean norm (tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag, tBodyGyroJerkMag). 
Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag. (Note the 'f' to indicate frequency domain signals). 

These signals were used to estimate variables of the feature vector for each pattern:  
'-XYZ' is used to denote 3-axial signals in the X, Y and Z directions.

* tBodyAcc-XYZ  (Numeric/Double)
* tGravityAcc-XYZ (Numeric/Double)
* tBodyAccJerk-XYZ  (Numeric/Double)
* tBodyGyro-XYZ (Numeric/Double)
* tBodyGyroJerk-XYZ (Numeric/Double)
* tBodyAccMag (Numeric/Double)
* tGravityAccMag  (Numeric/Double)
* tBodyAccJerkMag (Numeric/Double)
* tBodyGyroMag  (Numeric/Double)
* tBodyGyroJerkMag  (Numeric/Double)
* fBodyAcc-XYZ  (Numeric/Double)
* fBodyAccJerk-XYZ  (Numeric/Double)
* fBodyGyro-XYZ (Numeric/Double)
* fBodyAccMag (Numeric/Double)
* fBodyAccJerkMag (Numeric/Double)
* fBodyGyroMag  (Numeric/Double)
* fBodyGyroJerkMag  (Numeric/Double)

The set of variables that were estimated from these signals are: 

* mean(): Mean value
* std(): Standard deviation
* mad(): Median absolute deviation 
* max(): Largest value in array
* min(): Smallest value in array
* sma(): Signal magnitude area
* energy(): Energy measure. Sum of the squares divided by the number of values. 
* iqr(): Interquartile range 
* entropy(): Signal entropy
* arCoeff(): Autorregresion coefficients with Burg order equal to 4
* correlation(): correlation coefficient between two signals
* maxInds(): index of the frequency component with largest magnitude
* meanFreq(): Weighted average of the frequency components to obtain a mean frequency
* skewness(): skewness of the frequency domain signal 
* kurtosis(): kurtosis of the frequency domain signal 
* bandsEnergy(): Energy of a frequency interval within the 64 bins of the FFT of each window.
* angle(): Angle between to vectors.

Additional vectors obtained by averaging the signals in a signal window sample. These are used on the angle() variable:

* gravityMean (Numeric/Integer)
* tBodyAccMean  (Numeric/Integer)
* tBodyAccJerkMean  (Numeric/Integer)
* tBodyGyroMean (Numeric/Integer)
* tBodyGyroJerkMean (Numeric/Integer)

=== License and acknowledgements

Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012.

This dataset is distributed AS-IS and no responsibility implied or explicit can be addressed to the authors or their institutions for its use or misuse. Any commercial use is prohibited.
Jorge L. Reyes-Ortiz, Alessandro Ghio, Luca Oneto, Davide Anguita. November 2012.

=== Observations (Raw data)
* Even the subject number goes up to 30 there's only 21 subjects.
* The quantity of observations per each activity is not the same for all subjects. In other words if there are four observations for any subject in "WALKING" activity there are more o less than four observations in other activities.
* The files with the name y_train and y_test suits the activities performed for each subject(It is assumed the number of observations in y_train and y_test is the same for x_train and x_test respectively).
* Even the angle variables were calculated with the mean of other variables (gravityMean, tBodyAccMean, tBodyAccJerkMean, tBodyAccJerkMean, tBodyGyroMean, tBodyGyroMean, tBodyGyroJerkMean) they're not used to concieve the tidy data set

== Meta-data (Tidy data)
 The tidy data set will be created from the raw data mentioned above. The tidy data set will include the mean value and the standard
 deviation for the following features (There are only two new columns that differ from the inital raw data set: the activity descrption "Activity_desc"
 and the subject number or code "Subject_num" that are located in different files)

* Activity_desc (String)
* Subject_num (Numeric/Integer)
* tBodAcc (Numeric/Double)
* tGravityAcc (Numeric/Double)
* tBodAccJerk (Numeric/Double)
* tBodGyro  (Numeric/Double)
* tBodGyroJerk  (Numeric/Double)
* tBodAccMag  (Numeric/Double)
* tGravityAccMag  (Numeric/Double)
* tBodAccJerkMag  (Numeric/Double)
* tBodGyroMag (Numeric/Double)
* tBodGyroJerkMag (Numeric/Double)
* fBodAcc (Numeric/Double)
* fBodAccFreq (Numeric/Double)
* fBodAccJerk (Numeric/Double)
* fBodAccJerkFreq (Numeric/Double)
* fBodGyro  (Numeric/Double)
* fBodGyroFreq  (Numeric/Double)
* fBodAccMag  (Numeric/Double)
* fBodAccMagFreq  (Numeric/Double)
* fBodBodAccJerkMag (Numeric/Double)
* fBodBodAccJerkMagFreq (Numeric/Double)
* fBodBodGyroMag  (Numeric/Double)
* fBodBodGyroMagFreq  (Numeric/Double)
* fBodBodGyroJerkMag  (Numeric/Double)
* fBodBodGyroJerkMagFreq  (Numeric/Double)

== How the run_analisys script works?
=== Parameters

This function has only one parameter and it's the path where the UCI HAR Dataset directory is located. If the parameter is ommited
the "getwd" function is used to get the default working directory configured in R.

=== Local variables

* df_actdstr 
Data frame of activities dataset in the train directory (y_train.txt)
* df_subdstr
Data frame of subjects dataset in the train directory (subject_train.txt)
* df_valdstr
Data frame of values dataset in the train directory (x_train.txt)
* df_actdste: 
Data frame of activities dataset in the test directory (y_test.txt)
* df_subdste
Data frame of subjects dataset in the test directory (subject_test.txt)
* df_valdste
Data frame of values dataset in the test directory (x_test.txt)
* df_acticom
Data frame of values dataset in the test directory (x_test.txt)
* df_varname
Data frame for variable label
* df_data
Data frame for the complete data set (test and train)
* df_actlb
Data frame for activities labeles
* df_joacttr (joacttr - joined activities train)    
Data frame for activities matched with the training observations
* df_joactte (joactte - joined activities test)
Data frame for activities matched with the test observations
* df_josucom (josucom - joined subjects complete)
Data frame for subject list
* gr_subacmen
Data frame used to store grouped and summarized tidy data by activities and subjects

=== Process
* Invoking Libraries
The code starts with a notification, then it loads the libraries. After that it uses the suppressMessages() function
in order to avoid messeges displaying when calling libraries. 
    message("Executing! please wait...")
    suppressMessages(library(dplyr))
    suppressMessages(library(plyr))
    suppressMessages(library(base))
    
* Step 1. Verifies the files existence
It uses the paste() function to concatenate the extracted working directory with the specific path of the file. Also
it uses file.exists from the base package to verify the existence of the files. as it returns a boolean value the if sentence
will validate the result and depending on it (if the file is not in the specified path), the program will display a message and finish the execution.
  if(!file.exists(paste(c(rds_path,"/UCI HAR Dataset/test/subject_test.txt"),collapse=''))){
    message("subject_test.txt is not available! Please check the directory path.")
    return(FALSE)
  }
  
* Step 2. Check the number of rows for y_train(Activites), subject_train and x_train)
It uses the paste() function to concatenate the extracted working directory with the specific path of the file. Once the path is complete
it will be used as parameter for the read.table function to extract the data from the files.
  df_actdstr <- read.table(paste(c(rds_path,"/UCI HAR Dataset/train/y_train.txt"),collapse=''))
  df_subdstr <- read.table(paste(c(rds_path,"/UCI HAR Dataset/train/subject_train.txt"),collapse=''))  
  df_valdstr <- read.table(paste(c(rds_path,"/UCI HAR Dataset/train/x_train.txt"),collapse=''))
  
After extracting data, the program verifies the number of the observations is the same in all the files using the nrow() function (If not the program
returns false and stops the execution)
  if(nrow(df_actdstr) != nrow(df_subdstr) | nrow(df_valdstr) != nrow(df_subdstr)){
    message("The files in train directory don't have the same number of observations!")
    return(FALSE)
  }
  
* Step 3. Check the number of rows for y_test(Activites), subject_test and x_test)
As explained in step 2 it uses the paste() function to concatenate the extracted working directory with the specific path of the file. Once the path is complete
it will be used as parameter for the read.table function to extract the data from the files. The difference in this case is that it will extract
the data from the test files.

  df_actdste <- read.table(paste(c(rds_path,"/UCI HAR Dataset/test/y_test.txt"),collapse=''))
  df_subdste <- read.table(paste(c(rds_path,"/UCI HAR Dataset/test/subject_test.txt"),collapse=''))
  df_valdste <- read.table(paste(c(rds_path,"/UCI HAR Dataset/test/x_test.txt"),collapse=''))
  
fter extracting data the program verifies the number of the observations is the same in all the files using the nrow() function (If not the program
returns false and stops the execution)
  if(nrow(df_actdste) != nrow(df_subdste) | nrow(df_valdste) != nrow(df_subdste)){
    message("The files in test directory don't have the same number of observations!")
    return(FALSE)
  }
  
* Step 4. Binds (Merges) the training and the test sets to create one data set
It uses the rbind() function to combine the rows (observatios) from the test and train files, then it assigns this combination
to a new variable called df_acticom.
  df_acticom <-rbind(df_valdstr,df_valdste)
  
* Step 5. Set the Variable(column) names
It opens the features files, extracts the data and creates a data frame called df_varname, then it will extract the second column
with the feature names. With the sententece gsub("[[:punct:]]", "", df_varname) using the string "[[:punct:]]" as a parameter it wil
remove all the punctuation characters (such as "()" and " ") in order to use legible names (no errors by using the select function or else giving a column name with pharentesis) when a query is required.
Finally the column names or features are assigned to the complete dataset with the colnames() function.
  df_varname <- read.table(paste(c(rds_path,"/UCI HAR Dataset/features.txt"),collapse=''))
  df_varname <- df_varname$V2
  df_varname <- gsub("[[:punct:]]", "", df_varname)
  colnames(df_acticom) <- df_varname
  
* Step 6. Extracts only the measurements on the mean and standard deviation for each measurement
The program removes the columns that are not necessary and turns the data frame into a table data frame so the select() function can 
be used to extract only the columns with a substring "mean" or "std" in their names using  the contains() function.
  df_data <- df_acticom[,-(461:502)]
  df_data <- df_data[,-(513:519)]
  df_data <- tbl_df(df_data)
  df_data <- cbind(select(df_data,contains("mean",ignore.case = TRUE)),select(df_data,contains("std",ignore.case = TRUE)))

* Step 7. Changes de activities code number by their description
In order to change the activities codes by their respective description, the program uses the join() function. This function 
will match the codes with the descriptions for each observation. After it creates the data frame the colnames() function
is usted to set the column names.

  df_actlb <- read.table(paste(c(rds_path,"/UCI HAR Dataset/activity_labels.txt"),collapse=''))
  df_joacttr <- suppressMessages(join(df_actlb,df_actdstr))
  df_joactte <- suppressMessages(join(df_actlb,df_actdste))
  df_acticom <- rbind(df_joacttr,df_joactte)
  colnames(df_acticom) <- c("Activity_code","Activity_desc")
  
* Step 8. Merges the activites with the data set
Once the data frame of activites is ready it combines the activities description with the complete data set.
  df_data <- cbind(select(df_acticom,Activity_desc),df_data)
  
* Step 9. Extract the subjects and sets the Variable (column) names
Combines the subject codes of the train and test files and sets a column name for the data set called
df_josucom.
  df_josucom <- rbind(df_subdstr,df_subdste)
  colnames(df_josucom) <- c("Subject_num")

* Step 10. Merge the subjects with the data set
  df_data <- cbind(select(df_josucom,Subject_num),df_data
  
* Step 11. Create the file with the tidy data
It uses the chaining operator "%>%" in order to gather a couple sentences that will group and sumarize the data
for the output file. Finally the write.table() function creates a file with the second tidy data set.
  gr_subacmen <- df_data %>%
  group_by(Activity_desc,Subject_num) %>%
  summarise_each(funs(mean))
  message(paste(c("Creating secondary tidy data set in ",rds_path,"/tidymeanset.txt"),collapse=''))
  write.table(gr_subacmen, file = paste(c(rds_path,"/tidymeanset.txt"),collapse=''),row.names = FALSE)
  message("Process succesfully finished!")
  return(df_data)
  
== Authors
Created by Ronald Rodr√≠guez (ronaldraxon)
